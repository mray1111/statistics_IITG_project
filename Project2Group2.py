# -*- coding: utf-8 -*-
"""Stats_proj_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ctf58ICvIXpfGURo_OzvYqPB2RG-aMv
"""

import pandas as pd
import statsmodels.api as sm
import seaborn as sns
import numpy as np
from sklearn.preprocessing import StandardScaler
from statsmodels.graphics.gofplots import qqplot
import matplotlib.pyplot as plt

df = pd.read_csv("/content/House Price India.csv")

df.head(5)

"""Dropping not important data for making dataset simple"""

drop_cols = ["id", "Date", "Renovation Year","Lattitude", "Longitude","waterfront present","Postal Code"]
df.drop(drop_cols,axis = 1,inplace=True)

"""## Dividing the data into Regressors and predictors"""

y = df["Price"]
x = df.drop("Price",axis = 1)

"""Shape of the target and regressors"""

x.shape

y.shape

"""First I have scaled the data for less computation cost. Both the regressors and dependent variable columns are scaled using Standard Scaler"""

from sklearn.preprocessing import StandardScaler

# Create StandardScaler objects
scaler_x = StandardScaler()
scaler_y = StandardScaler()

# Fit scalers to the data
scaler_x.fit(x)
scaler_y.fit(y.values.reshape(-1, 1))  # Reshape y to a 2D array

# Transform the data
x_scaled = scaler_x.transform(x)
y_scaled = scaler_y.transform(y.values.reshape(-1, 1))

# Convert scaled data to DataFrames
x_scaled = pd.DataFrame(x_scaled, columns=x.columns)
y_scaled = pd.DataFrame(y_scaled, columns=["y_scaled"])

print(x_scaled.shape,y_scaled.shape)

"""Followed by Linear Regression to fit the model take output as the model coefficients β and the intercept β_0

"""

from sklearn.linear_model import LinearRegression

# Create linear regression object
model = LinearRegression()

# Fit the model
model.fit(x_scaled, y_scaled)

# Print coefficients and intercept
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# Predict y values
y_pred = model.predict(x_scaled)
y_pred

"""**Computed SSres and SSreg**"""

SSres[0]

SSreg[0]

# Add constant for intercept
X = sm.add_constant(x_scaled)

# Fit OLS model
model = sm.OLS(y_scaled, X).fit()

# # Print model summary
print(model.summary())

"""**Test for Significance of
Regression**
"""

p = x_scaled.shape[1]
F0 = SSreg[0]*(n-p-1)/(SSres[0]*p)
F0

"""Note: Null Hypothesis is rejected. So atleast one beta_j is not zero

**Hat Matrix calculation for further process**
"""

import numpy as np

def hat_matrix(X):
    # Convert X to numpy array
    X = X.values

    # Compute X^TX
    XTX = X.T @ X

    # Compute inverse of XTX
    XTX_inv = np.linalg.inv(XTX)

    # Compute X(X^TX)^{-1}X^T
    H = X @ XTX_inv @ X.T

    return H


H = hat_matrix(x_scaled)
print(H)

# Residuals
residuals = y_scaled - y_pred

# Squared residuals
squared_residuals = residuals ** 2

# Mean squared residuals
MSRes = squared_residuals.mean()

# Diagonal elements of the hat matrix
hii = np.diag(H)

# Standardized residuals
standardized_residuals = residuals / MSRes ** 0.5
n = H.shape[0]
a = np.diag(np.eye(n))


ss_residuals = standardized_residuals['y_scaled'].values
residual = residuals['y_scaled'].values


# Studentized residuals
studentized_residuals = []
for i in range(n):
  studentized_residuals.append(residuals['y_scaled'][i] / (MSRes[0] * (a - hii))[i] ** 0.5)

studentized_residuals = np.array(studentized_residuals)
residuals = np.array(residuals['y_scaled'])

ss_residuals

studentized_residuals

residuals

"""residuals, standardized residuals, studentized residuals are the following 3 types of residuals

**QQ Plot**
"""

# Create Q-Q plots for each type of residuals
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Q-Q plot for studentized residuals
qqplot(studentized_residuals, line='s', ax=axes[0])
axes[0].set_title('Q-Q Plot for Studentized Residuals')
# Q-Q plot for residuals
qqplot(residual, line='s', ax=axes[1])
axes[1].set_title('Q-Q Plot for Residuals')

# Q-Q plot for squared residuals
qqplot(np.sqrt(ss_residuals), line='s', ax=axes[2])  # Taking square root of ss_residuals for QQ plot
axes[2].set_title('Q-Q Plot for Square Root of SS Residuals')

plt.tight_layout()
plt.show()

"""**Plot of residual against fitted values**"""

# Plot of residual against fitted values
plt.figure(figsize=(10, 5))
plt.scatter(y_pred, residuals, alpha=0.5)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel("Fitted values")
plt.ylabel("Residuals")
plt.title("Residual Plot: Residuals vs Fitted Values")
plt.show()

"""Results Heteroscedascity"""

import matplotlib.pyplot as plt
x = x_scaled
# Residual plots against each regressor
fig, axes = plt.subplots(len(x.columns), 1, figsize=(10, 5*len(x.columns)), sharex=True)

for i, col in enumerate(x.columns):
    axes[i].scatter(x[col], residuals, alpha=0.5)
    axes[i].axhline(y=0, color='red', linestyle='--')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel("Residuals")
    axes[i].set_title(f"Residual Plot: Residuals vs {col}")

plt.tight_layout()
plt.show()

columns = x_scaled.columns
m = columns.shape[0]
for i in range(m):
  xj = columns[i]
  # Step 1: Regress y on all independent variables except x_j to obtain the residuals
  model_y = sm.OLS(y, x.drop(xj, axis=1))
  results_y = model_y.fit()
  residuals_y = results_y.resid

  # Step 2: Regress x_j on all independent variables except x_j to obtain the residuals
  model_xj = sm.OLS(x[xj], x.drop(xj, axis=1))
  results_xj = model_xj.fit()
  residuals_xj = results_xj.resid

  # Step 3: Plot residuals of y against residuals of x_j
  plt.figure(figsize=(8, 6))
  plt.scatter(residuals_xj, residuals_y, alpha=0.5)
  plt.xlabel("Residuals of "+xj)
  plt.ylabel("Residuals of y")
  plt.title("Partial Regression Plot: Residuals of y vs Residuals of "+xj)
  plt.grid(True)
  plt.show()

"""## Subset Selections

All Possible Regression method for subset selection
"""

import itertools
import statsmodels.api as sm

def all_possible_regression(X, y):
    best_model = None
    best_score = float('-inf')

    for k in range(1, len(X.columns) + 1):
        for subset in itertools.combinations(X.columns, k):
            X_subset = X[list(subset)]
            X_subset = sm.add_constant(X_subset)
            model = sm.OLS(y, X_subset).fit()
            if model.rsquared_adj > best_score:
                best_model = model
                best_score = model.rsquared_adj

    return best_model

# Usage
best_model = all_possible_regression(x_scaled, y_scaled)
print(best_model.summary())

"""Forward Selection Method for subset Selection"""

lreg = LinearRegression()
def forward_selection(X, y):
    remaining_predictors = set(X.columns)
    selected_predictors = []
    best_score = float('-inf')
    old_score = 0

    while remaining_predictors:
        scores = []
        for predictor in remaining_predictors:
            model = sm.OLS(y, sm.add_constant(X[selected_predictors + [predictor]])).fit()
            scores.append((predictor, model.rsquared_adj))

        best_predictor, best_score = max(scores, key=lambda x: x[1])
        if best_score - old_score >1e-03:
          selected_predictors.append(best_predictor)
          remaining_predictors.remove(best_predictor)
          old_score = best_score
        else:
          break


    return sm.OLS(y, sm.add_constant(X[selected_predictors])).fit(),selected_predictors

# Usage
best_model_forward,selected_predictors = forward_selection(x_scaled, y_scaled)
#print(best_model_forward.summary())
print(selected_predictors)

x_forward = x[selected_predictors]
x_forward.head()

"""**Correlation Matrix of the subset obtained using Forward Selection for multicollinearity check**"""

x_forward.corr()

sns.heatmap(x_forward.corr()>0.7, cmap= 'gray')

"""Living area with grade of the house and with number of bathrooms column has significant correlation."""

best_model_forward.rsquared_adj

"""So in case of the forward selelction the adjusted r-squared value is 0.646070793580188 and in case of all possible regression adjusted r-squared value is 0.648.



"""